{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "#load pre-trained word2vec model\n",
    "model_w2v = Word2Vec.load('E:/TempWorkDir/DEVICE/w2v_test/wiki.en.word2vec.model')\n",
    "word_vectors = model_w2v.wv\n",
    "del(model_w2v)\n",
    "#print(word_vectors['airplane'])\n",
    "#print(model_w2v['cat'])\n",
    "print(len(word_vectors['cat']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 400)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_14 to have shape (None, 512) but got array with shape (50000, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-dc183ed93bbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         shuffle=True)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1572\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1573\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1574\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1575\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1576\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1409\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1410\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1411\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1412\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1413\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    151\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_14 to have shape (None, 512) but got array with shape (50000, 1)"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "#load pre-trained word2vec model\n",
    "model_w2v = Word2Vec.load('E:/TempWorkDir/DEVICE/w2v_test/wiki.en.word2vec.model')\n",
    "word_vectors = model_w2v.wv\n",
    "del(model_w2v)\n",
    "\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.constraints import unit_norm\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sess = K.get_session()\n",
    "\n",
    "#load cifar10 data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "#a label to vector transform...\n",
    "lab2vec = []\n",
    "#num_label [text_label, vector]\n",
    "text_label = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "for text in text_label:\n",
    "    #entry = (text, word_vectors[text])\n",
    "    #lab2vec.append(entry)\n",
    "    lab2vec.append(word_vectors[text])\n",
    "\n",
    "lab2vec = np.asarray(lab2vec)\n",
    "print(lab2vec.shape)\n",
    "lab2vec = tf.constant(lab2vec)\n",
    "\n",
    "model = load_model('E:/TempWorkDir/DEVICE/saved_models/keras_cifar10_trained_model.h5')\n",
    "\"\"\"\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "###Pop these four layers from previous training\n",
    "\"\"\"\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "\n",
    "model.outputs = [model.layers[-1].output]\n",
    "model.layers[-1].outbound_nodes = []\n",
    "\n",
    "#model.layers.pop()\n",
    "#print(model.layers[-1].get_weights())\n",
    "#model.add(Dense(400, kernel_constraint=unit_norm()))\n",
    "#M = Dense(400, kernel_constraint=unit_norm())\n",
    "\n",
    "#transformation layer\n",
    "M = tf.truncated_normal((400, 512), stddev=0.1)\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    margin = tf.constant(0.1)\n",
    "    #outer loop counter\n",
    "    i = tf.constant(0)\n",
    "    #outer loop condition function\n",
    "    c = lambda i, _: tf.less(i, tf.shape(y_true)[0])\n",
    "    outer_sum_loss = tf.constant(0.0)\n",
    "    def process_ele(i, outer_sum_loss):\n",
    "        y_true_one = y_true[i]\n",
    "        y_pred_one = y_pred[i]\n",
    "        #word vector of true label\n",
    "        tvec = tf.reshape(tf.gather_nd(lab2vec, tf.cast(y_true_one, dtype=tf.int32)), [1, 400])\n",
    "        \n",
    "        #inner loop counter\n",
    "        ii = tf.constant(0)\n",
    "        #inner loop condition function\n",
    "        cc = lambda i, _: tf.less(i, tf.shape(lab2vec)[0])\n",
    "        inner_sum_loss = tf.constant(0.0)\n",
    "        def sum_over_label(ii, inner_sum_loss):\n",
    "            #word vector of false label\n",
    "            tnot = tf.reshape(tf.gather_nd(lab2vec, [ii]), [1, 400])\n",
    "            #M * image vector\n",
    "            M_mul_image = tf.matmul(M, tf.reshape(y_pred_one, [512, 1]))\n",
    "            #part of loss function\n",
    "            test1 = tf.reduce_sum(tf.matmul(tvec, M_mul_image))\n",
    "            test2 = tf.reduce_sum(tf.matmul(tnot, M_mul_image))           \n",
    "            return tf.add(ii, 1), tf.add(inner_sum_loss, K.relu(margin - test1 + test2))\n",
    "        _, inner_sum_loss = tf.while_loop(cc, sum_over_label, [ii, inner_sum_loss])\n",
    "        inner_sum_loss -= margin\n",
    "        #return average loss\n",
    "        return tf.add(i, 1), tf.add(outer_sum_loss, inner_sum_loss / tf.cast(tf.shape(lab2vec)[0], dtype=tf.float32))\n",
    "    _, outer_sum_loss = tf.while_loop(c, process_ele, [i, outer_sum_loss])\n",
    "    #return average loss over batch\n",
    "    return outer_sum_loss / tf.cast(tf.shape(y_true)[0], dtype=tf.float32)\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \n",
    "    pass\n",
    "    \n",
    "\n",
    "#optimizer = keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=True)\n",
    "\n",
    "model.compile(loss = loss,\n",
    "              optimizer = opt,\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=100,\n",
    "        validation_data=(x_test, y_test),\n",
    "        shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.relu(-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "i = tf.constant(0)\n",
    "c = lambda i: tf.less(i, 9)\n",
    "def b(i):\n",
    "    #print(i)\n",
    "    return tf.add(i, 1)\n",
    "r = tf.while_loop(c, b, [i])\n",
    "print(sess.run(r))\n",
    "print(i.eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "#load pre-trained word2vec model\n",
    "model_w2v = KeyedVectors.load_word2vec_format('E:/TempWorkDir/DEVICE/w2v_test/GoogleNews-vectors-negative300-SLIM.bin', binary=True)\n",
    "#word_vectors = model_w2v.wv\n",
    "#print(word_vectors['airplane'])\n",
    "#print(model_w2v['cat'])\n",
    "#print(word_vectors['cat'][:10])\n",
    "#print(word_vectors['dog'][:10])\n",
    "a = model_w2v['dog']\n",
    "#print(a)\n",
    "model_w2v.similar_by_vector(a - 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "Epoch 1/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 13.9585\n",
      "Epoch: 0, Error: 68.75738686695695\n",
      "\n",
      "391/391 [==============================] - 51s 131ms/step - loss: 13.9347\n",
      "Epoch 2/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 3.3184\n",
      "Epoch: 1, Error: 69.1428828719072\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 3.3185\n",
      "Epoch 3/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.8041\n",
      "Epoch: 2, Error: 70.02232686476782\n",
      "\n",
      "391/391 [==============================] - 51s 130ms/step - loss: 2.8038\n",
      "Epoch 4/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.6916\n",
      "Epoch: 3, Error: 67.980966177769\n",
      "\n",
      "391/391 [==============================] - 51s 130ms/step - loss: 2.6915\n",
      "Epoch 5/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.6344\n",
      "Epoch: 4, Error: 66.9223902283702\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 372s 952ms/step - loss: 0.6328\n",
      "Epoch 6/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1071\n",
      "Epoch: 5, Error: 67.80498806899413\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.1068\n",
      "Epoch 7/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1221\n",
      "Epoch: 6, Error: 69.1604019915685\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.1218\n",
      "Epoch 8/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0828\n",
      "Epoch: 7, Error: 70.35812559281476\n",
      "\n",
      "391/391 [==============================] - 51s 131ms/step - loss: 0.0828\n",
      "Epoch 9/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0499\n",
      "Epoch: 8, Error: 70.23235484375618\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0502\n",
      "Epoch 10/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0666\n",
      "Epoch: 9, Error: 69.42219464527443\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 384s 982ms/step - loss: 0.0664\n",
      "Epoch 11/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0673\n",
      "Epoch: 10, Error: 70.90062664379366\n",
      "\n",
      "391/391 [==============================] - 51s 129ms/step - loss: 0.0674\n",
      "Epoch 12/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1162\n",
      "Epoch: 11, Error: 72.96836015000008\n",
      "\n",
      "391/391 [==============================] - 51s 131ms/step - loss: 0.1160\n",
      "Epoch 13/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0897\n",
      "Epoch: 12, Error: 75.67796509992331\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.0895\n",
      "Epoch 14/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0886\n",
      "Epoch: 13, Error: 76.22560307336971\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.0884\n",
      "Epoch 15/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0395\n",
      "Epoch: 14, Error: 76.42058664816432\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 393s 1s/step - loss: 0.0394\n",
      "Epoch 16/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0869\n",
      "Epoch: 15, Error: 77.02986740274355\n",
      "\n",
      "391/391 [==============================] - 53s 135ms/step - loss: 0.0867\n",
      "Epoch 17/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0909\n",
      "Epoch: 16, Error: 79.15082486765459\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.0907\n",
      "Epoch 18/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1012\n",
      "Epoch: 17, Error: 82.11869871988893\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.1009\n",
      "Epoch 19/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0407\n",
      "Epoch: 18, Error: 80.93758381390944\n",
      "\n",
      "391/391 [==============================] - 51s 130ms/step - loss: 0.0407\n",
      "Epoch 20/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0503\n",
      "Epoch: 19, Error: 82.72286100313067\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 371s 950ms/step - loss: 0.0502\n",
      "Epoch 21/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0615\n",
      "Epoch: 20, Error: 80.73309942428023\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0613\n",
      "Epoch 22/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0538\n",
      "Epoch: 21, Error: 82.05645953328349\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0536\n",
      "Epoch 23/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0309\n",
      "Epoch: 22, Error: 82.87603967124596\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0309\n",
      "Epoch 24/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0352\n",
      "Epoch: 23, Error: 83.37383151636459\n",
      "\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 0.0351\n",
      "Epoch 25/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2726\n",
      "Epoch: 24, Error: 84.05262066237628\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 371s 950ms/step - loss: 0.2719\n",
      "Epoch 26/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0530\n",
      "Epoch: 25, Error: 84.92830884456635\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.0534\n",
      "Epoch 27/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0381\n",
      "Epoch: 26, Error: 85.38810356450267\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.0394\n",
      "Epoch 28/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0839\n",
      "Epoch: 27, Error: 85.97685076948255\n",
      "\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 0.0836\n",
      "Epoch 29/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0512\n",
      "Epoch: 28, Error: 90.6514136758633\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0511\n",
      "Epoch 30/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2092\n",
      "Epoch: 29, Error: 88.22111419891007\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 372s 951ms/step - loss: 0.2086\n",
      "Epoch 31/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0553\n",
      "Epoch: 30, Error: 87.16818480798975\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0552\n",
      "Epoch 32/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1048\n",
      "Epoch: 31, Error: 88.99583765328862\n",
      "\n",
      "391/391 [==============================] - 55s 141ms/step - loss: 0.1045\n",
      "Epoch 33/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0570\n",
      "Epoch: 32, Error: 89.19921429012902\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0587\n",
      "Epoch 34/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0414\n",
      "Epoch: 33, Error: 89.07571845897473\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0413\n",
      "Epoch 35/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0360\n",
      "Epoch: 34, Error: 91.42159312753938\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 372s 950ms/step - loss: 0.0359\n",
      "Epoch 36/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0941\n",
      "Epoch: 35, Error: 87.96636567311361\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0980\n",
      "Epoch 37/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1509\n",
      "Epoch: 36, Error: 88.14253220078535\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.1505\n",
      "Epoch 38/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1393\n",
      "Epoch: 37, Error: 92.61376781528816\n",
      "\n",
      "391/391 [==============================] - 51s 129ms/step - loss: 0.1390\n",
      "Epoch 39/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0604\n",
      "Epoch: 38, Error: 90.68872334808111\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0602\n",
      "Epoch 40/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0409\n",
      "Epoch: 39, Error: 93.49026025063358\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 369s 944ms/step - loss: 0.0408\n",
      "Epoch 41/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0641\n",
      "Epoch: 40, Error: 91.72464146930724\n",
      "\n",
      "391/391 [==============================] - 51s 130ms/step - loss: 0.0640\n",
      "Epoch 42/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0410\n",
      "Epoch: 41, Error: 92.6530482203234\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0409\n",
      "Epoch 43/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0284\n",
      "Epoch: 42, Error: 92.83075004070997\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0325\n",
      "Epoch: 43, Error: 89.36017963197082\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0324\n",
      "Epoch 45/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0243\n",
      "Epoch: 44, Error: 91.22637670929544\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 370s 946ms/step - loss: 0.0242\n",
      "Epoch 46/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4725\n",
      "Epoch: 45, Error: 94.95112352375872\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.4731\n",
      "Epoch 47/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1528\n",
      "Epoch: 46, Error: 95.22226587776095\n",
      "\n",
      "391/391 [==============================] - 51s 130ms/step - loss: 0.1524\n",
      "Epoch 48/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0772\n",
      "Epoch: 47, Error: 92.54676743107848\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.0770\n",
      "Epoch 49/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0246\n",
      "Epoch: 48, Error: 93.65785904508084\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0246\n",
      "Epoch 50/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0305\n",
      "Epoch: 49, Error: 94.805335030891\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 373s 954ms/step - loss: 0.0305\n",
      "Epoch 51/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0381\n",
      "Epoch: 50, Error: 95.94770543882623\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.0380\n",
      "Epoch 52/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0332\n",
      "Epoch: 51, Error: 96.1487868335098\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0331\n",
      "Epoch 53/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0139\n",
      "Epoch: 52, Error: 94.89594647078775\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0138\n",
      "Epoch 54/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0528\n",
      "Epoch: 53, Error: 93.5142957563512\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0526\n",
      "Epoch 55/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1557\n",
      "Epoch: 54, Error: 93.31639589671977\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 370s 947ms/step - loss: 0.1553\n",
      "Epoch 56/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1450\n",
      "Epoch: 55, Error: 98.3121208329685\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.1447\n",
      "Epoch 57/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0730\n",
      "Epoch: 56, Error: 98.66398890712298\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0728\n",
      "Epoch 58/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1277\n",
      "Epoch: 57, Error: 99.60358600039035\n",
      "\n",
      "391/391 [==============================] - 51s 131ms/step - loss: 0.1274\n",
      "Epoch 59/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1124\n",
      "Epoch: 58, Error: 102.54126349836588\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.1129\n",
      "Epoch 60/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3087\n",
      "Epoch: 59, Error: 96.11753568728454\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 372s 951ms/step - loss: 0.3079\n",
      "Epoch 61/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0498\n",
      "Epoch: 60, Error: 97.53519579186104\n",
      "\n",
      "391/391 [==============================] - 51s 129ms/step - loss: 0.0497\n",
      "Epoch 62/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0409\n",
      "Epoch: 61, Error: 98.18319540307857\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0408\n",
      "Epoch 63/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0390\n",
      "Epoch: 62, Error: 96.4671348426491\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0389\n",
      "Epoch 64/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0386\n",
      "Epoch: 63, Error: 98.12580035603605\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0385\n",
      "Epoch 65/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0667\n",
      "Epoch: 64, Error: 102.47845734702423\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 372s 951ms/step - loss: 0.0665\n",
      "Epoch 66/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0739\n",
      "Epoch: 65, Error: 101.34312106878497\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0737\n",
      "Epoch 67/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0325\n",
      "Epoch: 66, Error: 100.13735570455901\n",
      "\n",
      "391/391 [==============================] - 52s 132ms/step - loss: 0.0326\n",
      "Epoch 68/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1307\n",
      "Epoch: 67, Error: 102.8171598627232\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.1315\n",
      "Epoch 69/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3831\n",
      "Epoch: 68, Error: 101.77022137795575\n",
      "\n",
      "391/391 [==============================] - 51s 131ms/step - loss: 0.3821\n",
      "Epoch 70/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2266\n",
      "Epoch: 69, Error: 107.91349510848522\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 418s 1s/step - loss: 0.2260\n",
      "Epoch 71/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.6576\n",
      "Epoch: 70, Error: 101.76179536641575\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.6576\n",
      "Epoch 72/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3048\n",
      "Epoch: 71, Error: 105.35957247717306\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.3046\n",
      "Epoch 73/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2903\n",
      "Epoch: 72, Error: 108.28333686920814\n",
      "\n",
      "391/391 [==============================] - 51s 131ms/step - loss: 0.2901\n",
      "Epoch 74/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2935\n",
      "Epoch: 73, Error: 104.83082689903677\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.2933\n",
      "Epoch 75/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2477\n",
      "Epoch: 74, Error: 104.95715773361735\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 370s 947ms/step - loss: 0.2475\n",
      "Epoch 76/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2381\n",
      "Epoch: 75, Error: 106.13078293274157\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.2380\n",
      "Epoch 77/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2443\n",
      "Epoch: 76, Error: 106.27075952710584\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.2442\n",
      "Epoch 78/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.6767\n",
      "Epoch: 77, Error: 109.93793145101517\n",
      "\n",
      "391/391 [==============================] - 51s 130ms/step - loss: 0.6947\n",
      "Epoch 79/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.7954\n",
      "Epoch: 78, Error: 109.4130069417879\n",
      "\n",
      "391/391 [==============================] - 51s 130ms/step - loss: 1.7924\n",
      "Epoch 80/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1632\n",
      "Epoch: 79, Error: 109.53850008593872\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 371s 948ms/step - loss: 0.1627\n",
      "Epoch 81/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0626\n",
      "Epoch: 80, Error: 107.79428246617317\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.0631\n",
      "Epoch 82/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0772\n",
      "Epoch: 81, Error: 109.0459987972863\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0770\n",
      "Epoch 83/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0585\n",
      "Epoch: 82, Error: 105.94575913064182\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0589\n",
      "Epoch 84/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.2550\n",
      "Epoch: 83, Error: 106.15538060246035\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 1.2539\n",
      "Epoch 85/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3105\n",
      "Epoch: 84, Error: 107.29890785645694\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 371s 948ms/step - loss: 0.3102\n",
      "Epoch 86/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2482\n",
      "Epoch: 85, Error: 108.37112651811913\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.2487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3346\n",
      "Epoch: 86, Error: 106.06129434960894\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.3343\n",
      "Epoch 88/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2710\n",
      "Epoch: 87, Error: 106.14679356524721\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.2708\n",
      "Epoch 89/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4925\n",
      "Epoch: 88, Error: 105.98348844377324\n",
      "\n",
      "391/391 [==============================] - 51s 130ms/step - loss: 0.4913\n",
      "Epoch 90/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0444\n",
      "Epoch: 89, Error: 106.10713156452402\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 372s 952ms/step - loss: 0.0443\n",
      "Epoch 91/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0495\n",
      "Epoch: 90, Error: 107.56120883114636\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.0494\n",
      "Epoch 92/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.5074\n",
      "Epoch: 91, Error: 107.97419582074508\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.5061\n",
      "Epoch 93/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0974\n",
      "Epoch: 92, Error: 108.70709596201777\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.0971\n",
      "Epoch 94/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0609\n",
      "Epoch: 93, Error: 108.9622185039334\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.0615\n",
      "Epoch 95/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0406\n",
      "Epoch: 94, Error: 108.81288467394188\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 370s 947ms/step - loss: 0.0405\n",
      "Epoch 96/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1608\n",
      "Epoch: 95, Error: 109.28725952235982\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.1746\n",
      "Epoch 97/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4303\n",
      "Epoch: 96, Error: 106.93467264110222\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.4292\n",
      "Epoch 98/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.5384\n",
      "Epoch: 97, Error: 107.86650497745723\n",
      "\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.5376\n",
      "Epoch 99/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4089\n",
      "Epoch: 98, Error: 108.09676520526409\n",
      "\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.4079\n",
      "Epoch 100/100\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.0861\n",
      "Epoch: 99, Error: 108.36360480403528\n",
      "\n",
      "Model saved!\n",
      "391/391 [==============================] - 367s 938ms/step - loss: 0.0858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20104e0b7f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################\n",
    "batch_size = 128\n",
    "w2v_model_path = '../w2v_test/GoogleNews-vectors-negative300-SLIM.bin'\n",
    "vistual_model_path = '../saved_models/keras_cifar10_trained_model.h5'\n",
    "lr = 0.0005\n",
    "\n",
    "#Read pre-trained word2vec model from Google news\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format(w2v_model_path, binary=True)\n",
    "#Get vector dimension\n",
    "word_dim = word_vectors['the'].shape[0]\n",
    "\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.constraints import unit_norm\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#load cifar10 data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "train_size = x_train.shape[0]\n",
    "val_size = x_test.shape[0]\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "#Transform text to vector\n",
    "lab2vec = []\n",
    "text_label = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "for text in text_label:\n",
    "    lab2vec.append(word_vectors[text])\n",
    "    \n",
    "#Transform label (y) to word vector\n",
    "y_vec = np.empty([y_train.shape[0], word_dim])\n",
    "for a in range(y_train.shape[0]):\n",
    "    y_vec[a] = word_vectors[text_label[y_train[a][0]]]\n",
    "    \n",
    "def get_train_batch():\n",
    "    # A function yield a batch of training data\n",
    "    while 1:\n",
    "        counter = 0\n",
    "        len_train = x_train.shape[0]\n",
    "        while (counter + 1) * batch_size < len_train:\n",
    "            start = counter * batch_size\n",
    "            end = (counter + 1) * batch_size\n",
    "            yield x_train[start:end], y_vec[start:end]\n",
    "        yield x_train[counter * batch_size:len_train], y_vec[counter * batch_size:len_train]\n",
    "\n",
    "lab2vec_array = np.asarray(lab2vec)\n",
    "lab2vec = tf.constant(lab2vec_array)\n",
    "\n",
    "#Load a pre-trained vistual model\n",
    "model = load_model(vistual_model_path)\n",
    "\"\"\"\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "###Pop these four layers from previous training\n",
    "\"\"\"\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "\n",
    "#Add transformation layer\n",
    "model.add(Dense(word_dim, use_bias=False))\n",
    "\n",
    "#Set transformation layer as network output\n",
    "model.outputs = [model.layers[-1].output]\n",
    "model.layers[-1].outbound_nodes = []\n",
    "\n",
    "#Custom loss function\n",
    "def loss(y_true, y_pred):\n",
    "    margin = 0.1\n",
    "    #loop counter\n",
    "    i = tf.constant(0)\n",
    "    #loop condition function\n",
    "    c = lambda i, _: tf.less(i, tf.shape(y_true)[0])\n",
    "    outer_sum_loss = tf.constant(0.0)\n",
    "    def process_ele(i, outer_sum_loss):\n",
    "        #Get a subtensor from batch\n",
    "        y_true_one = y_true[i]\n",
    "        y_pred_one = y_pred[i]\n",
    "\n",
    "        #Stack margin to a 10*1 matrix\n",
    "        margin_stack = tf.reshape(tf.stack([tf.constant(0.1)] * 10), [10, 1])\n",
    "        #Stack true label to a word_dim*10 matrix and transpose it\n",
    "        y_true_one_stack = tf.stack([tf.transpose(y_true_one)] * 10)\n",
    "        #Reshape predict from (word_dim,) to (word_dim,1)\n",
    "        y_pred_one_t = tf.reshape(y_pred_one, [word_dim, 1])\n",
    "        #Calculate loss\n",
    "        r = margin_stack - tf.matmul(y_true_one_stack, y_pred_one_t) + tf.matmul(lab2vec, y_pred_one_t)\n",
    "        #Summation\n",
    "        #We did not exclude true label inside summation, so we subtract extra margin\n",
    "        sum_inner_loss = tf.reduce_sum(K.relu(r)) - margin\n",
    "        #Return counter++ and accumulated loss\n",
    "        return tf.add(i, 1), tf.add(outer_sum_loss, sum_inner_loss)\n",
    "    \n",
    "    _, outer_sum_loss = tf.while_loop(c, process_ele, [i, outer_sum_loss])\n",
    "    #Return average loss over batch\n",
    "    #return outer_sum_loss / tf.cast(tf.shape(y_true)[0], dtype=tf.float32)\n",
    "    return outer_sum_loss\n",
    "\n",
    "def val_batch_gen():\n",
    "    #Yield a batch of validation data\n",
    "    val_len = x_test.shape[0]\n",
    "    counter = 0\n",
    "    while((counter + 1) * batch_size < val_len):\n",
    "        start = counter * batch_size\n",
    "        end = (counter + 1) * batch_size\n",
    "        yield x_test[start:end], y_test[start:end]\n",
    "        counter += 1\n",
    "    yield x_test[counter * batch_size:val_len], y_test[counter * batch_size:val_len]\n",
    "    \n",
    "class evaluate(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        print('init')\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print()\n",
    "        #Log error and top K info to text file\n",
    "        loglog = open(\"./log.txt\", \"a\")\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            k = 10\n",
    "            topn_hit = np.zeros(k)\n",
    "            for batch_x, batch_y in val_batch_gen():\n",
    "                y_pred = model.predict(batch_x)\n",
    "                for i in range(y_pred.shape[0]):\n",
    "                    #Retrive top k vector similar to model output\n",
    "                    vec = word_vectors.similar_by_vector(y_pred[i], topn=k)\n",
    "                    for ii in range(len(vec)):\n",
    "                        if vec[ii][0] == text_label[batch_y[i][0]]:\n",
    "                            #Count right number\n",
    "                            topn_hit[ii:] += 1\n",
    "                            break\n",
    "            #Compute ratio\n",
    "            topn_hit = topn_hit / val_size\n",
    "            #Write to text file\n",
    "            loglog.write('Epoch: {0}, TopK: {1}\\n'.format(epoch, topn_hit))\n",
    "            #print('Epoch: {0}, TopK: {1}\\n'.format(epoch, topn_hit))\n",
    "        \n",
    "        \n",
    "        sumsum = 0.0\n",
    "        #Also log distance between true vector and predicted vector\n",
    "        for i in range(x_test.shape[0]):\n",
    "            x = x_test[i].reshape((1, 32, 32, 3))\n",
    "            y = lab2vec_array[y_test[i][0]]\n",
    "            y_pred = model.predict(x)\n",
    "            sumsum += ((y - y_pred) ** 2).mean()\n",
    "        print('Epoch: {0}, Error: {1}\\n'.format(epoch, sumsum))\n",
    "        loglog.write('Epoch: {0}, Error: {1}\\n'.format(epoch, sumsum))\n",
    "        \n",
    "        #Save model every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model.save('./model/devise_epoch_' + str(epoch + 1) + '.h5')\n",
    "            print('Model saved!')\n",
    "        loglog.close()            \n",
    "\n",
    "#opt = keras.optimizers.rmsprop(lr=lr)\n",
    "#opt = keras.optimizers.SGD(lr=lr, momentum=0.9, decay=1e-4, nesterov=True)\n",
    "opt = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "model.compile(loss = loss, optimizer = opt)\n",
    "\n",
    "tbcb = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "    \n",
    "model.fit_generator(get_train_batch(),\n",
    "                    steps_per_epoch=train_size // batch_size + 1,\n",
    "                    epochs=100,\n",
    "                    callbacks=[evaluate(), tbcb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from PIL import Image\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\"\"\"\n",
    "index = 1111\n",
    "x = x_train[index]\n",
    "y = y_train[index]\n",
    "img = Image.fromarray(x, 'RGB')\n",
    "#img.save('my.png')\n",
    "print(y)\n",
    "img.show()\n",
    "\"\"\"\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.relu(-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "i = tf.constant(0)\n",
    "c = lambda i: tf.less(i, 9)\n",
    "def b(i):\n",
    "    #print(i)\n",
    "    return tf.add(i, 1)\n",
    "r = tf.while_loop(c, b, [i])\n",
    "print(sess.run(r))\n",
    "print(i.eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant(0.2)\n",
    "b = tf.stack([a]*5)\n",
    "c = tf.shape(b)\n",
    "tf.Session().run(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.asarray([1,2,3,4,5])\n",
    "a[3:] += 1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 1234\n",
    "i == 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(3,2)\n",
    "slice_first = lambda x: x[0:2 , :]\n",
    "print(a)\n",
    "print('-----')\n",
    "print(slice_first(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "w2v_model_path = '../w2v_test/GoogleNews-vectors-negative300-SLIM.bin'\n",
    "vistual_model_path = '../saved_models/keras_cifar10_trained_model.h5'\n",
    "lr = 0.001\n",
    "\n",
    "#Read pre-trained word2vec model from Google news\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format(w2v_model_path, binary=True)\n",
    "#Get vector dimension\n",
    "word_dim = word_vectors['the'].shape[0]\n",
    "\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.constraints import unit_norm\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#load cifar10 data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "train_size = x_train.shape[0]\n",
    "val_size = x_test.shape[0]\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "#Transform text to vector\n",
    "lab2vec = []\n",
    "text_label = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "for text in text_label:\n",
    "    lab2vec.append(word_vectors[text])\n",
    "    \n",
    "#Transform label (y) to word vector\n",
    "y_vec = np.empty([y_train.shape[0], word_dim])\n",
    "for a in range(y_train.shape[0]):\n",
    "    y_vec[a] = word_vectors[text_label[y_train[a][0]]]\n",
    "\n",
    "\"\"\"\n",
    "def get_train_batch():\n",
    "    # A function yield a batch of training data\n",
    "    while 1:\n",
    "        counter = 0\n",
    "        len_train = x_train.shape[0]\n",
    "        while (counter + 1) * batch_size < len_train:\n",
    "            start = counter * batch_size\n",
    "            end = (counter + 1) * batch_size\n",
    "            yield x_train[start:end], y_vec[start:end]\n",
    "        yield x_train[counter * batch_size:len_train], y_vec[counter * batch_size:len_train]\n",
    "\"\"\"    \n",
    "\n",
    "def get_train_batch():\n",
    "    i = 0\n",
    "    while 1:\n",
    "        yield x_train[i].reshape((1, 32, 32, 3)), y_vec[i].reshape((1, word_dim))\n",
    "        i = (i + 1) % train_size\n",
    "\n",
    "\n",
    "lab2vec_array = np.asarray(lab2vec)\n",
    "lab2vec = tf.constant(lab2vec_array)\n",
    "\n",
    "#Load a pre-trained vistual model\n",
    "model = load_model(vistual_model_path)\n",
    "\"\"\"\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "###Pop these four layers from previous training\n",
    "\"\"\"\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "\n",
    "#Add transformation layer\n",
    "model.add(Dense(word_dim, use_bias=False))\n",
    "\n",
    "#Set transformation layer as network output\n",
    "model.outputs = [model.layers[-1].output]\n",
    "model.layers[-1].outbound_nodes = []\n",
    "\n",
    "#Custom loss function\n",
    "def loss(y_true, y_pred):\n",
    "    margin = 0.1\n",
    "    \"\"\"\n",
    "    #loop counter\n",
    "    i = tf.constant(0)\n",
    "    #loop condition function\n",
    "    c = lambda i, _: tf.less(i, tf.shape(y_true)[0])\n",
    "    outer_sum_loss = tf.constant(0.0)\n",
    "    def process_ele(i, outer_sum_loss):\n",
    "        #Get a subtensor from batch\n",
    "        y_true_one = y_true[i]\n",
    "        y_pred_one = y_pred[i]\n",
    "\n",
    "        #Stack margin to a 10*1 matrix\n",
    "        margin_stack = tf.reshape(tf.stack([tf.constant(0.1)] * 10), [10, 1])\n",
    "        #Stack true label to a word_dim*10 matrix and transpose it\n",
    "        y_true_one_stack = tf.stack([tf.transpose(y_true_one)] * 10)\n",
    "        #Reshape predict from (word_dim,) to (word_dim,1)\n",
    "        y_pred_one_t = tf.reshape(y_pred_one, [word_dim, 1])\n",
    "        #Calculate loss\n",
    "        r = margin_stack - tf.matmul(y_true_one_stack, y_pred_one_t) + tf.matmul(lab2vec, y_pred_one_t)\n",
    "        #Summation\n",
    "        #We did not exclude true label inside summation, so we subtract extra margin\n",
    "        sum_inner_loss = tf.reduce_sum(K.relu(r)) - margin\n",
    "        #Return counter++ and accumulated loss\n",
    "        return tf.add(i, 1), tf.add(outer_sum_loss, sum_inner_loss)\n",
    "    \n",
    "    _, outer_sum_loss = tf.while_loop(c, process_ele, [i, outer_sum_loss])\n",
    "    #Return average loss over batch\n",
    "    return outer_sum_loss / tf.cast(tf.shape(y_true)[0], dtype=tf.float32)\n",
    "    \"\"\"\n",
    "    #Stack margin to a 10*1 matrix\n",
    "    margin_stack = tf.reshape(tf.stack([tf.constant(0.1)] * 10), [10, 1])\n",
    "    #Stack true label to a word_dim*10 matrix and transpose it\n",
    "    y_true_stack = tf.stack([tf.transpose(y_true[0])] * 10)\n",
    "    #Reshape predict from (word_dim,) to (word_dim,1)\n",
    "    y_pred_t = tf.reshape(y_pred, [word_dim, 1])\n",
    "    #Calculate loss\n",
    "    r = margin_stack - tf.matmul(y_true_stack, y_pred_t) + tf.matmul(lab2vec, y_pred_t)\n",
    "    #Summation\n",
    "    #We did not exclude true label inside summation, so we subtract extra margin\n",
    "    sum_loss = tf.reduce_sum(K.relu(r)) - margin\n",
    "    #Return counter++ and accumulated loss\n",
    "    return sum_loss\n",
    "\"\"\"\n",
    "def val_batch_gen():\n",
    "    #Yield a batch of validation data\n",
    "    val_len = x_test.shape[0]\n",
    "    counter = 0\n",
    "    while((counter + 1) * batch_size < val_len):\n",
    "        start = counter * batch_size\n",
    "        end = (counter + 1) * batch_size\n",
    "        yield x_test[start:end], y_test[start:end]\n",
    "        counter += 1\n",
    "    yield x_test[counter * batch_size:val_len], y_test[counter * batch_size:val_len]\n",
    "\"\"\"\n",
    "    \n",
    "class evaluate(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        print('init')\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print()\n",
    "        #Log error and top K info to text file\n",
    "        loglog = open(\"./log.txt\", \"a\")\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            k = 10\n",
    "            topn_hit = np.zeros(k)\n",
    "            for x, y in zip(x_test, y_test):\n",
    "                y_pred = model.predict(x)\n",
    "                #Retrive top k vector similar to model output\n",
    "                vec = word_vectors.similar_by_vector(y_pred, topn=k)\n",
    "                for ii in range(len(vec)):\n",
    "                    if vec[ii][0] == text_label[y[0]]:\n",
    "                        #Count right number\n",
    "                        topn_hit[ii:] += 1\n",
    "                        break\n",
    "            #Compute ratio\n",
    "            topn_hit = topn_hit / val_size\n",
    "            #Write to text file\n",
    "            loglog.write('Epoch: {0}, TopK: {1}\\n'.format(epoch, topn_hit))\n",
    "        \n",
    "        sumsum = 0.0\n",
    "        #Also log distance between true vector and predicted vector\n",
    "        for x, y in zip(x_test, y_test):\n",
    "            x = x.reshape((1, 32, 32, 3))\n",
    "            y = lab2vec_array[y[0]]\n",
    "            y_pred = model.predict(x)\n",
    "            sumsum += ((y - y_pred) ** 2).mean()\n",
    "        loglog.write('Epoch: {0}, Error: {1}\\n'.format(epoch, sumsum / val_size))\n",
    "        \n",
    "        #Save model every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model.save('./model/devise_epoch_' + str(epoch + 1) + '.h5')\n",
    "            print('Model saved!')\n",
    "        loglog.close()            \n",
    "\n",
    "opt = keras.optimizers.rmsprop(lr=lr)\n",
    "#opt = keras.optimizers.SGD(lr=0.001, momentum=0.9, decay=1e-6, nesterov=True)\n",
    "#opt = keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(loss = loss, optimizer = opt)\n",
    "\n",
    "tbcb = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "    \n",
    "model.fit_generator(get_train_batch(),\n",
    "                    steps_per_epoch=train_size,\n",
    "                    epochs=100,\n",
    "                    callbacks=[evaluate(), tbcb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
